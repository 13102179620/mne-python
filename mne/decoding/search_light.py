# Author: Jean-Remi King <jeanremi.king@gmail.com>
#
# License: BSD (3-clause)

import numpy as np

from .mixin import TransformerMixin
from .base import BaseEstimator  # XXX why are these copied in weird places. We should just have an externals.sklearn, not manually copy/pasted objects  # noqa
from ..parallel import parallel_func


class SearchLight(BaseEstimator, TransformerMixin):
    """Search Light
    Fit, predict and score a series of models to each subset of the
    dataset along the third dimension.

    Parameters
    ----------
    base_estimator : object | None (default=None)
        The base estimator to iteratively fit on a subset of the dataset. If
        None, then the estimator is a Logistic Regression.
    n_jobs : int, optional (default=1)
        The number of jobs to run in parallel for both `fit` and `predict`.
        If -1, then the number of jobs is set to the number of cores.
    """
    def __init__(self, base_estimator=None, n_jobs=1):
        from sklearn.linear_model import LogisticRegression
        self.base_estimator = (LogisticRegression() if base_estimator is None
                               else base_estimator)
        self.n_jobs = n_jobs
        if not isinstance(self.n_jobs, int):
            raise ValueError('n_jobs must be int, got %s' % n_jobs)

    def fit_transform(self, X, y):
        """
        Fit and transform a series of independent estimators to the dataset.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The training input samples. For each iteration, a clone estimator
            is fitted independently.
        y : array, shape(n_samples,)
            The target values.

        Returns
        -------
        y_pred : array, shape(n_samples, n_iterations)
            Predicted values for each estimator.
        """
        return self.fit(X, y).transform(X)

    def fit(self, X, y):
        """Fit a series of independent estimators to the dataset.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The training input samples. For each iteration, a clone estimator
            is fitted independently.
        y : array, shape(n_samples,)
            The target values.

        Returns
        -------
        self : object
            Return self.
        """
        self.estimators_ = list()
        parallel, p_func, n_jobs = parallel_func(_sl_fit, self.n_jobs)
        estimators = parallel(
            p_func(self.base_estimator, split, y)
            for split in np.array_split(X, n_jobs, axis=2))
        self.estimators_ = np.concatenate(estimators, 0)
        return self

    def _transform(self, X, method):
        """Aux. function to make parallel predictions/transformation"""
        parallel, p_func, n_jobs = parallel_func(_sl_transform, self.n_jobs)
        X_splits = np.array_split(X, n_jobs, axis=2)
        est_splits = np.array_split(self.estimators_, n_jobs)
        y_pred = parallel(p_func(est, x, method)
                          for (est, x) in zip(est_splits, X_splits))

        if n_jobs > 1:
            y_pred = np.concatenate(y_pred, axis=1)
        else:
            y_pred = y_pred[0]
        return y_pred

    def transform(self, X):
        """Transform the dataset with a series of independent estimators.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The input samples. For each iteration, the corresponding estimator
            makes a transformation of the data:
            e.g. [estimators[ii].transform(X[:, :, ii])
                  for ii in range(n_iteration)]

        Returns
        -------
        Xt : array, shape(n_samples, n_iterations)
            Transformed values generated by each estimator.
        """
        if not hasattr(self.base_estimator, 'transform'):
            ValueError('self.base_estimator does not have `transform` method.')
        return self._transform(X, 'transform')

    def predict(self, X):
        """Predict the dataset with a series of independent estimators.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The input samples. For each iteration, the corresponding estimator
            makes the sample predictions:
            e.g. [estimators[ii].predict(X[:, :, ii])
                  for ii in range(n_iteration)]

        Returns
        -------
        y_pred : array, shape(n_samples, n_iterations)
            Predicted values for each estimator/iteration.
        """
        if not hasattr(self.base_estimator, 'predict'):
            ValueError('self.base_estimator does not have `predict` method.')
        return self._transform(X, 'predict')

    def predict_proba(self, X):
        """Predict the dataset with a series of independent estimators.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The input samples. For each iteration, the corresponding estimator
            makes the sample probabilistic predictions:
            e.g. [estimators[ii].predict_proba(X[:, :, ii])
                  for ii in range(n_iteration)]

        Returns
        -------
        y_pred : array, shape(n_samples, n_iterations, n_categories)
            Predicted probabilities for each estimator/iteration.
        """
        if not hasattr(self.base_estimator, 'predict_proba'):
            ValueError('self.base_estimator does not have `predict_proba` '
                       'method.')
        return self._transform(X, 'predict_proba')

    def decision_function(self, X):
        """Distances of the samples X to the separating hyperplanes.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The input samples. For each iteration, the corresponding estimator
            makes the sample probabilistic predictions:
            e.g. [estimators[ii].decision_function(X[:, :, ii])
                  for ii in range(n_iteration)]

        Returns
        -------
        y_pred : array, shape(n_samples, n_iterations, n_classes * (n_classes-1) / 2)  # noqa
            Predicted distances for each estimator/iteration.
        """
        if not hasattr(self.base_estimator, 'decision_function'):
            ValueError('self.base_estimator does not have `decision_function` '
                       'method.')
        return self._transform(X, 'decision_function')


def _sl_fit(estimator, X, y):
    """Aux. function to fit search lights in parallel"""
    from sklearn.base import clone
    estimators_ = list()
    for ii in range(X.shape[2]):
        est = clone(estimator)
        est.fit(X[:, :, ii], y)
        estimators_.append(est)
    return estimators_


def _sl_transform(estimators, X, method):
    """Aux. function to transform search lights in parallel"""
    n_sample, n_chan, n_iter = X.shape
    y_pred = np.array((n_sample, n_iter))
    for ii, est in enumerate(estimators):
        transform = getattr(est, method)
        _y_pred = transform(X[:, :, ii])
        # init predictions
        if ii == 0:
            y_pred = _sl_init_pred(_y_pred, X)
        y_pred[:, ii, ...] = _y_pred
    return y_pred


def _sl_init_pred(y_pred, X):
    """Aux. function to initialize search lights predictions"""
    n_sample, n_chan, n_iter = X.shape
    if y_pred.ndim > 1:
        # for estimator that generate multidimensional y_pred,
        # e.g. clf.predict_proba()
        y_pred = np.zeros(np.r_[n_sample, n_iter, y_pred.shape[1:]])
    else:
        # for estimator that generate unidimensional y_pred,
        # e.g. clf.predict()
        y_pred = np.zeros((n_sample, n_iter))
    return y_pred


class GeneralizationLight(SearchLight):
    """Generalization Light

    Fit a search-light and use them to apply a systematic cross-feature
    generalization.

    Parameters
    ----------
    estimator : object | None (default=None)
        The base estimator to iteratively fit on a subset of the dataset. If
        None, then the estimator is a Logistic Regression.
    n_jobs : int, optional (default=1)
        The number of jobs to run in parallel for both `fit` and `predict`.
        If -1, then the number of jobs is set to the number of cores.
    """

    def _transform(self, X, method):
        """Aux. function to make parallel predictions/transformation"""
        parallel, p_func, n_jobs = parallel_func(_gl_transform, self.n_jobs)
        y_pred = parallel(
            p_func(self.estimators_, x_split, method)
            for x_split in np.array_split(X, n_jobs, axis=2))

        y_pred = np.concatenate(y_pred, axis=2)
        return y_pred

    def transform(self, X):
        """Transform the dataset with a series of independent estimators.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The input samples. For each iteration, the corresponding estimator
            generates a transformation of X.

        Returns
        -------
        Xt : array, shape(n_samples, n_train_iterations, n_test_iterations)
            Transformed values generated by each estimator.
        """
        method = 'transform'
        if not hasattr(self.base_estimator, 'transform'):
            method = 'predict'
        self._transform(X, method)

    def predict(self, X):
        """Predict the dataset with a series of independent estimators.

        Parameters
        ----------
        X : array, shape(n_samples, n_features, n_iteration)
            The training input samples. For each iteration, a clone estimator
            is fitted independently.

        Returns
        -------
        y_pred : array, shape(n_samples, n_train_iterations, n_test_iterations)
            Predicted values for each estimator.
        """
        return self._transform(X, 'predict')

    def predict_proba(self, X):
        if not hasattr(self.base_estimator, 'predict_proba'):
            ValueError('self.base_estimator does not have `predict_proba` '
                       'method.')
        return self._transform(X, 'predict_proba')

    def decision_function(self, X):
        if not hasattr(self.base_estimator, 'decision_function'):
            ValueError('self.base_estimator does not have `decision_function` '
                       'method.')
        return self._transform(X, 'decision_function')

    def score(self, X, y):
        # FIXME: not sure how to design
        raise NotImplementedError()


def _gl_transform(estimators, X, method):
    """Transform the dataset by applying each estimator to all iteration of
    the data.

    Parameters
    ----------
    X : array, shape(n_samples, n_features, n_iteration)
        The training input samples. For each iteration, a clone estimator
        is fitted independently.

    Returns
    -------
    Xt : array, shape(n_samples, n_iterations)
        Transformed values generated by each estimator.
    """
    n_sample, n_chan, n_iter = X.shape
    for ii, est in enumerate(estimators):
        # stack generalized data for faster prediction
        X_stack = np.transpose(X, [1, 0, 2])
        X_stack = np.reshape(X_stack, [n_chan, n_sample * n_iter]).T
        transform = getattr(est, method)
        _y_pred = transform(X_stack)
        # unstack generalizations
        if _y_pred.ndim == 2:
            _y_pred = np.reshape(_y_pred, [n_sample, n_iter])
        else:
            shape = np.r_[n_sample, n_iter, _y_pred.shape[1:]]
            _y_pred = np.reshape(_y_pred, shape)
        # init
        if ii == 0:
            y_pred = _gl_init_pred(_y_pred, X, len(estimators))
        y_pred[:, ii, ...] = _y_pred
    return y_pred


def _gl_init_pred(y_pred, X, n_train):
    n_sample, n_chan, n_iter = X.shape
    if y_pred.ndim == 3:
        y_pred = np.zeros((n_sample, n_train, n_iter, y_pred.shape[-1]))
    else:
        y_pred = np.zeros((n_sample, n_train, n_iter))
    return y_pred
